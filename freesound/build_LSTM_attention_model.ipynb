{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, Embedding, MaxPooling2D, Input, Dense\n",
    "from keras.layers import Flatten, Reshape, Activation ,concatenate, Reshape, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "from keras.layers.recurrent import LSTM\n",
    "from lwlrap import tf_lwlrap\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_attention():\n",
    "    def __init__(self, num_freq = 128, len_div = 256, num_hidden=300):\n",
    "        self.num_freq = num_freq\n",
    "        self.len_div = len_div\n",
    "        self.num_hidden = num_hidden\n",
    "        \n",
    "    def LSTM(self):\n",
    "\n",
    "        self.inputs = Input(shape=(self.num_freq, self.len_div), name='input')\n",
    "\n",
    "        self.lstm = LSTM(self.num_hidden, return_sequences=False, name='LSTM')(self.inputs)\n",
    "        self.drop = Dropout(rate=0.05)(self.lstm)\n",
    "        self.dense = Dense(80, name='dense1')(self.drop)\n",
    "        self.pred = Activation('softmax',name='pred')(self.dense)\n",
    "\n",
    "        adam = optimizers.Adam(lr=0.0001)\n",
    "        model = Model(inputs=self.inputs, outputs=self.pred)\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[tf_lwlrap])\n",
    "        return model\n",
    "    \n",
    "    def Embed_LSTM(self):\n",
    "#         adam = optimizers.Adam(lr=0.0001)\n",
    "        model = Model(inputs=self.inputs, outputs=self.lstm)\n",
    "#         model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[tf_lwlrap])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_attn = LSTM_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_attn.LSTM()\n",
    "model2 = lstm_attn.Embed_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "LSTM (LSTM)                  (None, 300)               668400    \n",
      "=================================================================\n",
      "Total params: 668,400\n",
      "Trainable params: 668,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleNMT(pad_length=100,\n",
    "              n_chars=105,\n",
    "              n_labels=6,\n",
    "              embedding_learnable=False,\n",
    "              encoder_units=256,\n",
    "              decoder_units=256,\n",
    "              trainable=True,\n",
    "              return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Builds a Neural Machine Translator that has alignment attention\n",
    "    :param pad_length: the size of the input sequence\n",
    "    :param n_chars: the number of characters in the vocabulary\n",
    "    :param n_labels: the number of possible labelings for each character\n",
    "    :param embedding_learnable: decides if the one hot embedding should be refinable.\n",
    "    :return: keras.models.Model that can be compiled and fit'ed\n",
    "    *** REFERENCES ***\n",
    "    Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \n",
    "    \"Neural Machine Translation By Jointly Learning To Align and Translate\" \n",
    "    \"\"\"\n",
    "    input_ = Input(shape=(pad_length,), dtype='float32')\n",
    "    input_embed = Embedding(n_chars, n_chars,\n",
    "                            input_length=pad_length,\n",
    "                            trainable=embedding_learnable,\n",
    "                            weights=[np.eye(n_chars)],\n",
    "                            name='OneHot')(input_)\n",
    "\n",
    "    rnn_encoded = Bidirectional(LSTM(encoder_units, return_sequences=True),\n",
    "                                name='bidirectional_1',\n",
    "                                merge_mode='concat',\n",
    "                                trainable=trainable)(input_embed)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=rnn_encoded)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = simpleNMT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "OneHot (Embedding)           (None, 100, 105)          11025     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 512)          741376    \n",
      "=================================================================\n",
      "Total params: 752,401\n",
      "Trainable params: 741,376\n",
      "Non-trainable params: 11,025\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
