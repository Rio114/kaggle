{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split \n",
    "import librosa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_curated.csv', 'train_noisy.csv', 'sample_submission.csv', 'train_noisy', 'train_curated', 'test']\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = \"input/\"\n",
    "# INPUT_FOLDER = \"../input/\"\n",
    "print(os.listdir(INPUT_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CURATED_PATH = INPUT_FOLDER + \"train_curated.csv\"\n",
    "TRAIN_NOISY_PATH = INPUT_FOLDER + \"train_noisy.csv\"\n",
    "SAMPLE_SUBMISSION_PATH = INPUT_FOLDER + \"sample_submission.csv\"\n",
    "TRAIN_CURATED = INPUT_FOLDER + \"train_curated/\"\n",
    "TRAIN_NOISY = INPUT_FOLDER + \"train_noisy/\"\n",
    "TEST = INPUT_FOLDER + \"test/\"\n",
    "\n",
    "train_curated = pd.read_csv(TRAIN_CURATED_PATH)\n",
    "train_noisy = pd.read_csv(TRAIN_NOISY_PATH)\n",
    "sample = pd.read_csv(SAMPLE_SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, src_dict):\n",
    "    ar = np.zeros([len(labels), len(src_dict)])\n",
    "    for i, label in enumerate(labels):\n",
    "        label_list = label.split(',')\n",
    "        for la in label_list:\n",
    "            ar[i, src_dict[la]] = 1\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = sample.columns[1:]\n",
    "num_targets = len(target_names)\n",
    "\n",
    "src_dict = {target_names[i]:i for i in range(num_targets)}\n",
    "src_dict_inv = {i:target_names[i] for i in range(num_targets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_freq = 128\n",
    "len_div = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(num_freq,len_div,1), name='input')\n",
    "\n",
    "dense_list = []\n",
    "\n",
    "## Block 1\n",
    "conv1 = Conv2D(8, (19, 19),activation='relu',padding='same',name='conv1')(inputs)\n",
    "pool1 = MaxPooling2D((19, 19),strides=(2, 2),padding='same',name='pool1')(conv1)\n",
    "norm1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm1')(pool1)\n",
    "drop1 = Dropout(rate=0.05)(norm1)\n",
    "\n",
    "conv1_1 = Conv2D(8, (11, 11),activation='relu',padding='same',name='conv1_1')(drop1)\n",
    "pool1_1 = MaxPooling2D((11, 11),strides=(2, 2),padding='same',name='pool1_1')(conv1_1)\n",
    "norm1_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm1_1')(pool1_1)\n",
    "drop1_1 = Dropout(rate=0.05)(norm1_1)\n",
    "\n",
    "conv1_2 = Conv2D(8, (7, 7),activation='relu',padding='same',name='conv1_2')(drop1_1)\n",
    "pool1_2 = MaxPooling2D((7, 7),strides=(2, 2),padding='same',name='pool1_2')(conv1_2)\n",
    "norm1_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm1_2')(pool1_2)\n",
    "drop1_2 = Dropout(rate=0.05)(norm1_2)\n",
    "                       \n",
    "flatten1 = Flatten(name='flatten1')(drop1_2)\n",
    "dense1 = Dense(32, name='dense1')(flatten1)\n",
    "act1 = Activation('relu',name='act1')(dense1)\n",
    "dense_list.append(act1)\n",
    "\n",
    "## Block 2\n",
    "conv2 = Conv2D(8, (13, 13),activation='relu',padding='same',name='conv2')(inputs)\n",
    "pool2 = MaxPooling2D((13, 13), strides=(2, 2), padding='same',name='pool2')(conv2)\n",
    "norm2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm2')(pool2)\n",
    "drop2 = Dropout(rate=0.05)(norm2)\n",
    "\n",
    "conv2_1 = Conv2D(8, (11, 11),activation='relu',padding='same',name='conv2_1')(drop2)\n",
    "pool2_1 = MaxPooling2D((11, 11), strides=(2, 2), padding='same',name='pool2_1')(conv2_1)\n",
    "norm2_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm2_1')(pool2_1)\n",
    "drop2_1 = Dropout(rate=0.05)(norm2_1)\n",
    "\n",
    "conv2_2 = Conv2D(8, (7, 7),activation='relu',padding='same',name='conv2_2')(drop2_1)\n",
    "pool2_2 = MaxPooling2D((7, 7), strides=(2, 2), padding='same',name='pool2_2')(conv2_2)\n",
    "norm2_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm2_2')(pool2_2)\n",
    "drop2_2 = Dropout(rate=0.05)(norm2_2)\n",
    "\n",
    "flatten2 = Flatten(name='flatten2')(drop2_2)\n",
    "dense2 = Dense(32, name='dense2')(flatten2)\n",
    "act2 = Activation('relu',name='act2')(dense2)\n",
    "dense_list.append(act2)\n",
    "\n",
    "## Block 3\n",
    "conv3 = Conv2D(8, (11, 11), activation='relu',padding='same',name='conv3')(inputs)\n",
    "pool3 = MaxPooling2D((11, 11), strides=(2, 2), padding='same',name='pool3')(conv3)\n",
    "norm3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm3')(pool3)\n",
    "drop3 = Dropout(rate=0.05)(norm3)\n",
    "\n",
    "conv3_1 = Conv2D(8, (7, 7), activation='relu',padding='same',name='conv3_1')(drop3)\n",
    "pool3_1 = MaxPooling2D((7, 7), strides=(2, 2), padding='same',name='pool3_1')(conv3_1)\n",
    "norm3_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm3_1')(pool3_1)\n",
    "drop3_1 = Dropout(rate=0.05)(norm3_1)\n",
    "\n",
    "conv3_2 = Conv2D(8, (5, 5), activation='relu',padding='same',name='conv3_2')(drop3_1)\n",
    "pool3_2 = MaxPooling2D((5, 5), strides=(2, 2), padding='same',name='pool3_2')(conv3_2)\n",
    "norm3_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm3_2')(pool3_2)\n",
    "drop3_2 = Dropout(rate=0.05)(norm3)\n",
    "\n",
    "flatten3 = Flatten(name='flatten3')(drop3_2)\n",
    "dense3 = Dense(32, name='dense3')(flatten3)\n",
    "act3 = Activation('relu',name='act3')(dense3)\n",
    "dense_list.append(act3)\n",
    "\n",
    "## Block 4\n",
    "conv4 = Conv2D(8, (7, 7),activation='relu',padding='same',name='conv4')(inputs)\n",
    "pool4 = MaxPooling2D((7, 7), strides=(2, 2), padding='same',name='pool4')(conv4)\n",
    "norm4 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm4')(pool4)\n",
    "drop4 = Dropout(rate=0.05)(norm4)\n",
    "\n",
    "conv4_1 = Conv2D(8, (5, 5),activation='relu',padding='same',name='conv4_1')(drop4)\n",
    "pool4_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same',name='pool4_1')(conv4_1)\n",
    "norm4_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm4_1')(pool4_1)\n",
    "drop4_1 = Dropout(rate=0.05)(norm4_1)\n",
    "\n",
    "conv4_2 = Conv2D(8, (3, 3),activation='relu',padding='same',name='conv4_2')(drop4_1)\n",
    "pool4_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='same',name='pool4_2')(conv4_2)\n",
    "norm4_2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm4_2')(pool4_2)\n",
    "drop4_2 = Dropout(rate=0.05)(norm4_2)\n",
    "\n",
    "flatten4 = Flatten(name='flatten4')(drop4_2)\n",
    "dense4 = Dense(32, name='dense4')(flatten4)\n",
    "act4 = Activation('relu',name='act4')(dense4)\n",
    "dense_list.append(act4)\n",
    "\n",
    "concat = concatenate(dense_list, name='concat', axis=1)\n",
    "\n",
    "dense2 = Dense(80, name='dense_all')(concat)\n",
    "pred = Activation('softmax',name='pred')(dense2)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_one_sample_positive_class_precisions(y_true, y_pred) :\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    # find true labels\n",
    "    pos_class_indices = tf.where(y_true > 0) \n",
    "    \n",
    "    # put rank on each element\n",
    "    retrieved_classes = tf.nn.top_k(y_pred, k=num_classes).indices\n",
    "    sample_range = tf.zeros(shape=tf.shape(tf.transpose(y_pred)), dtype=tf.int32)\n",
    "    sample_range = tf.add(sample_range, tf.range(tf.shape(y_pred)[0], delta=1))\n",
    "    sample_range = tf.transpose(sample_range)\n",
    "    sample_range = tf.reshape(sample_range, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_classes = tf.reshape(retrieved_classes, (-1,num_classes*tf.shape(y_pred)[0]))\n",
    "    retrieved_class_map = tf.concat((sample_range, retrieved_classes), axis=0)\n",
    "    retrieved_class_map = tf.transpose(retrieved_class_map)\n",
    "    retrieved_class_map = tf.reshape(retrieved_class_map, (tf.shape(y_pred)[0], num_classes, 2))\n",
    "    \n",
    "    class_range = tf.zeros(shape=tf.shape(y_pred), dtype=tf.int32)\n",
    "    class_range = tf.add(class_range, tf.range(num_classes, delta=1))\n",
    "    \n",
    "    class_rankings = tf.scatter_nd(retrieved_class_map,\n",
    "                                          class_range,\n",
    "                                          tf.shape(y_pred))\n",
    "    \n",
    "    #pick_up ranks\n",
    "    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices)\n",
    "\n",
    "    # add one for division for \"presicion_at_hits\"\n",
    "    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n",
    "    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n",
    "    \n",
    "    # generate tensor [num_sample, predict_rank], \n",
    "    # top-N predicted elements have flag, N is the number of positive for each sample.\n",
    "    sample_label = pos_class_indices[:, 0]   \n",
    "    sample_label = tf.reshape(sample_label, (-1, 1))\n",
    "    sample_label = tf.cast(sample_label, tf.int32)\n",
    "    \n",
    "    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))\n",
    "    retrieved_class_true_position = tf.concat((sample_label, \n",
    "                                               num_correct_until_correct), axis=1)\n",
    "    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n",
    "    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n",
    "                                         retrieved_pos, \n",
    "                                         tf.shape(y_pred))\n",
    "    # cumulate predict_rank\n",
    "    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n",
    "\n",
    "    # find positive position\n",
    "    pos_ret_indices = tf.where(retrieved_class_true > 0)\n",
    "\n",
    "    # find cumulative hits\n",
    "    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n",
    "    correct_rank = tf.cast(correct_rank, tf.float32)\n",
    "\n",
    "    # compute presicion\n",
    "    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n",
    "\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def tf_lwlrap(y_true, y_pred):\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n",
    "    pos_flgs = tf.cast(y_true > 0, tf.int32)\n",
    "    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n",
    "    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n",
    "                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n",
    "    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n",
    "    class_label = pos_class_indices[:,1]\n",
    "    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n",
    "                                                        class_label,\n",
    "                                                       num_classes)\n",
    "    labels_per_class = tf.cast(labels_per_class, tf.float32)\n",
    "    labels_per_class = tf.add(labels_per_class, 1e-7)\n",
    "    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n",
    "                                  tf.cast(labels_per_class, tf.float32))\n",
    "    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 128, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 128, 256, 8)  2896        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 128, 256, 8)  1360        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 128, 256, 8)  400         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 64, 128, 8)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 64, 128, 8)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 64, 128, 8)   0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm1 (BatchNormalization)      (None, 64, 128, 8)   32          pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm2 (BatchNormalization)      (None, 64, 128, 8)   32          pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm4 (BatchNormalization)      (None, 64, 128, 8)   32          pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 128, 8)   0           norm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64, 128, 8)   0           norm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 128, 8)   0           norm4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 64, 128, 8)   7752        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 64, 128, 8)   7752        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                (None, 64, 128, 8)   1608        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_1 (MaxPooling2D)          (None, 32, 64, 8)    0           conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2_1 (MaxPooling2D)          (None, 32, 64, 8)    0           conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool4_1 (MaxPooling2D)          (None, 32, 64, 8)    0           conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm1_1 (BatchNormalization)    (None, 32, 64, 8)    32          pool1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm2_1 (BatchNormalization)    (None, 32, 64, 8)    32          pool2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm4_1 (BatchNormalization)    (None, 32, 64, 8)    32          pool4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 64, 8)    0           norm1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 64, 8)    0           norm2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 64, 8)    0           norm4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                (None, 32, 64, 8)    3144        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                (None, 32, 64, 8)    3144        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 128, 256, 8)  976         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2 (Conv2D)                (None, 32, 64, 8)    584         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_2 (MaxPooling2D)          (None, 16, 32, 8)    0           conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2_2 (MaxPooling2D)          (None, 16, 32, 8)    0           conv2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 64, 128, 8)   0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool4_2 (MaxPooling2D)          (None, 16, 32, 8)    0           conv4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm1_2 (BatchNormalization)    (None, 16, 32, 8)    32          pool1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm2_2 (BatchNormalization)    (None, 16, 32, 8)    32          pool2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm3 (BatchNormalization)      (None, 64, 128, 8)   32          pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "norm4_2 (BatchNormalization)    (None, 16, 32, 8)    32          pool4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 32, 8)    0           norm1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16, 32, 8)    0           norm2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64, 128, 8)   0           norm3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 32, 8)    0           norm4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten1 (Flatten)              (None, 4096)         0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten2 (Flatten)              (None, 4096)         0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten3 (Flatten)              (None, 65536)        0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten4 (Flatten)              (None, 4096)         0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           131104      flatten1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 32)           131104      flatten2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense3 (Dense)                  (None, 32)           2097184     flatten3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense4 (Dense)                  (None, 32)           131104      flatten4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "act1 (Activation)               (None, 32)           0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "act2 (Activation)               (None, 32)           0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "act3 (Activation)               (None, 32)           0           dense3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "act4 (Activation)               (None, 32)           0           dense4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 128)          0           act1[0][0]                       \n",
      "                                                                 act2[0][0]                       \n",
      "                                                                 act3[0][0]                       \n",
      "                                                                 act4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_all (Dense)               (None, 80)           10320       concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "pred (Activation)               (None, 80)           0           dense_all[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,530,752\n",
      "Trainable params: 2,530,592\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[tf_lwlrap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_dataset/train_arr_0.pickle', 'rb') as f:\n",
    "    X_train0 = pickle.load(f)\n",
    "    y_train0 = pickle.load(f)\n",
    "with open('preprocessed_dataset/train_arr_1.pickle', 'rb') as f:\n",
    "    X_train1 = pickle.load(f)\n",
    "    y_train1 = pickle.load(f)\n",
    "    \n",
    "X_train = np.vstack([X_train0, X_train1])\n",
    "y_train = np.vstack([y_train0, y_train1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_dataset/val_arr_0.pickle', 'rb') as f:\n",
    "    X_val0 = pickle.load(f)\n",
    "    y_val0 = pickle.load(f)\n",
    "with open('preprocessed_dataset/val_arr_1.pickle', 'rb') as f:\n",
    "    X_val1 = pickle.load(f)\n",
    "    y_val1 = pickle.load(f)\n",
    "    \n",
    "X_val = np.vstack([X_val0, X_val1])\n",
    "y_val = np.vstack([y_val0, y_val1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "           rotation_range=0,\n",
    "           width_shift_range=128,\n",
    "           height_shift_range=0,\n",
    "           shear_range=0,\n",
    "           zoom_range=0,\n",
    "           horizontal_flip=False,\n",
    "           vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7550 samples, validate on 1827 samples\n",
      "Epoch 1/30\n",
      "7550/7550 [==============================] - 96s 13ms/step - loss: 5.2323 - tf_lwlrap: 0.2227 - val_loss: 4.4877 - val_tf_lwlrap: 0.2837\n",
      "Epoch 2/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 3.9183 - tf_lwlrap: 0.3899 - val_loss: 3.9048 - val_tf_lwlrap: 0.3848\n",
      "Epoch 3/30\n",
      "7550/7550 [==============================] - 90s 12ms/step - loss: 3.2619 - tf_lwlrap: 0.5044 - val_loss: 3.7823 - val_tf_lwlrap: 0.4252\n",
      "Epoch 4/30\n",
      "7550/7550 [==============================] - 90s 12ms/step - loss: 2.7918 - tf_lwlrap: 0.5861 - val_loss: 3.3129 - val_tf_lwlrap: 0.4884\n",
      "Epoch 5/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 2.4566 - tf_lwlrap: 0.6522 - val_loss: 3.4375 - val_tf_lwlrap: 0.4939\n",
      "Epoch 6/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 2.2107 - tf_lwlrap: 0.6978 - val_loss: 3.4894 - val_tf_lwlrap: 0.4829\n",
      "Epoch 7/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 2.0278 - tf_lwlrap: 0.7344 - val_loss: 3.3990 - val_tf_lwlrap: 0.5148\n",
      "Epoch 8/30\n",
      "7550/7550 [==============================] - 90s 12ms/step - loss: 1.8484 - tf_lwlrap: 0.7693 - val_loss: 3.3659 - val_tf_lwlrap: 0.5175\n",
      "Epoch 9/30\n",
      "7550/7550 [==============================] - 90s 12ms/step - loss: 1.7036 - tf_lwlrap: 0.7940 - val_loss: 3.4327 - val_tf_lwlrap: 0.5260\n",
      "Epoch 10/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.5761 - tf_lwlrap: 0.8193 - val_loss: 3.3765 - val_tf_lwlrap: 0.5484\n",
      "Epoch 11/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.4491 - tf_lwlrap: 0.8435 - val_loss: 3.5014 - val_tf_lwlrap: 0.5448\n",
      "Epoch 12/30\n",
      "7550/7550 [==============================] - 90s 12ms/step - loss: 1.3673 - tf_lwlrap: 0.8587 - val_loss: 3.5473 - val_tf_lwlrap: 0.5479\n",
      "Epoch 13/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.3155 - tf_lwlrap: 0.8642 - val_loss: 3.4511 - val_tf_lwlrap: 0.5501\n",
      "Epoch 14/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.2342 - tf_lwlrap: 0.8846 - val_loss: 3.6741 - val_tf_lwlrap: 0.5556\n",
      "Epoch 15/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.1343 - tf_lwlrap: 0.9019 - val_loss: 3.4993 - val_tf_lwlrap: 0.5638\n",
      "Epoch 16/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.0999 - tf_lwlrap: 0.9066 - val_loss: 3.7904 - val_tf_lwlrap: 0.5549\n",
      "Epoch 17/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.0657 - tf_lwlrap: 0.9135 - val_loss: 3.7866 - val_tf_lwlrap: 0.5527\n",
      "Epoch 18/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 1.0165 - tf_lwlrap: 0.9256 - val_loss: 3.7759 - val_tf_lwlrap: 0.5523\n",
      "Epoch 19/30\n",
      "7550/7550 [==============================] - 92s 12ms/step - loss: 0.9743 - tf_lwlrap: 0.9303 - val_loss: 3.8798 - val_tf_lwlrap: 0.5488\n",
      "Epoch 20/30\n",
      "7550/7550 [==============================] - 92s 12ms/step - loss: 0.9255 - tf_lwlrap: 0.9388 - val_loss: 3.9744 - val_tf_lwlrap: 0.5620\n",
      "Epoch 21/30\n",
      "7550/7550 [==============================] - 92s 12ms/step - loss: 0.9101 - tf_lwlrap: 0.9417 - val_loss: 4.1844 - val_tf_lwlrap: 0.5513\n",
      "Epoch 22/30\n",
      "7550/7550 [==============================] - 92s 12ms/step - loss: 0.8900 - tf_lwlrap: 0.9473 - val_loss: 3.9472 - val_tf_lwlrap: 0.5707\n",
      "Epoch 23/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.8575 - tf_lwlrap: 0.9540 - val_loss: 4.0760 - val_tf_lwlrap: 0.5563\n",
      "Epoch 24/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.8130 - tf_lwlrap: 0.9612 - val_loss: 3.9867 - val_tf_lwlrap: 0.5557\n",
      "Epoch 25/30\n",
      "7550/7550 [==============================] - 92s 12ms/step - loss: 0.7908 - tf_lwlrap: 0.9603 - val_loss: 4.1005 - val_tf_lwlrap: 0.5625\n",
      "Epoch 26/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.8627 - tf_lwlrap: 0.9593 - val_loss: 4.3425 - val_tf_lwlrap: 0.5511\n",
      "Epoch 27/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.8077 - tf_lwlrap: 0.9636 - val_loss: 4.9277 - val_tf_lwlrap: 0.5439\n",
      "Epoch 28/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.8020 - tf_lwlrap: 0.9654 - val_loss: 4.1811 - val_tf_lwlrap: 0.5699\n",
      "Epoch 29/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.7425 - tf_lwlrap: 0.9717 - val_loss: 4.2981 - val_tf_lwlrap: 0.5628\n",
      "Epoch 30/30\n",
      "7550/7550 [==============================] - 91s 12ms/step - loss: 0.7273 - tf_lwlrap: 0.9774 - val_loss: 4.2243 - val_tf_lwlrap: 0.5560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9671e7de80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5429 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5429/5429 [==============================] - 88s 16ms/step - loss: 4.4068 - tf_lwlrap: 0.3063 - val_loss: 4.2941 - val_tf_lwlrap: 0.2845\n",
      "Epoch 2/3\n",
      "5429/5429 [==============================] - 86s 16ms/step - loss: 4.0473 - tf_lwlrap: 0.3662 - val_loss: 4.1904 - val_tf_lwlrap: 0.3207\n",
      "Epoch 3/3\n",
      "5429/5429 [==============================] - 86s 16ms/step - loss: 3.8026 - tf_lwlrap: 0.4172 - val_loss: 4.2961 - val_tf_lwlrap: 0.3116\n",
      "Train on 5878 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5878/5878 [==============================] - 92s 16ms/step - loss: 4.2658 - tf_lwlrap: 0.3405 - val_loss: 4.1644 - val_tf_lwlrap: 0.3129\n",
      "Epoch 2/3\n",
      "5878/5878 [==============================] - 91s 16ms/step - loss: 3.9426 - tf_lwlrap: 0.3900 - val_loss: 4.8870 - val_tf_lwlrap: 0.3200\n",
      "Epoch 3/3\n",
      "5878/5878 [==============================] - 92s 16ms/step - loss: 3.7666 - tf_lwlrap: 0.4298 - val_loss: 4.1775 - val_tf_lwlrap: 0.3293\n",
      "Train on 5856 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5856/5856 [==============================] - 91s 16ms/step - loss: 3.7409 - tf_lwlrap: 0.4278 - val_loss: 4.1952 - val_tf_lwlrap: 0.3352\n",
      "Epoch 2/3\n",
      "5856/5856 [==============================] - 91s 16ms/step - loss: 3.4261 - tf_lwlrap: 0.4874 - val_loss: 4.2494 - val_tf_lwlrap: 0.3353\n",
      "Epoch 3/3\n",
      "5856/5856 [==============================] - 91s 16ms/step - loss: 3.2457 - tf_lwlrap: 0.5250 - val_loss: 4.3163 - val_tf_lwlrap: 0.3412\n",
      "Train on 5888 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5888/5888 [==============================] - 91s 15ms/step - loss: 4.0813 - tf_lwlrap: 0.3641 - val_loss: 4.0597 - val_tf_lwlrap: 0.3379\n",
      "Epoch 2/3\n",
      "5888/5888 [==============================] - 91s 15ms/step - loss: 3.7412 - tf_lwlrap: 0.4202 - val_loss: 3.9994 - val_tf_lwlrap: 0.3586\n",
      "Epoch 3/3\n",
      "5888/5888 [==============================] - 91s 15ms/step - loss: 3.5582 - tf_lwlrap: 0.4577 - val_loss: 4.1119 - val_tf_lwlrap: 0.3513\n",
      "Train on 5899 samples, validate on 5750 samples\n",
      "Epoch 1/3\n",
      "5899/5899 [==============================] - 92s 16ms/step - loss: 4.0603 - tf_lwlrap: 0.3846 - val_loss: 4.0151 - val_tf_lwlrap: 0.3600\n",
      "Epoch 2/3\n",
      "5899/5899 [==============================] - 91s 15ms/step - loss: 3.7490 - tf_lwlrap: 0.4373 - val_loss: 3.9930 - val_tf_lwlrap: 0.3723\n",
      "Epoch 3/3\n",
      "5899/5899 [==============================] - 91s 15ms/step - loss: 3.5660 - tf_lwlrap: 0.4705 - val_loss: 4.1521 - val_tf_lwlrap: 0.3562\n",
      "Train on 5871 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5871/5871 [==============================] - 93s 16ms/step - loss: 4.0240 - tf_lwlrap: 0.4021 - val_loss: 3.9571 - val_tf_lwlrap: 0.3630\n",
      "Epoch 2/3\n",
      "5871/5871 [==============================] - 92s 16ms/step - loss: 3.7326 - tf_lwlrap: 0.4555 - val_loss: 4.0573 - val_tf_lwlrap: 0.3595\n",
      "Epoch 3/3\n",
      "5871/5871 [==============================] - 92s 16ms/step - loss: 3.5388 - tf_lwlrap: 0.4873 - val_loss: 3.9250 - val_tf_lwlrap: 0.3851\n",
      "Train on 5872 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5872/5872 [==============================] - 93s 16ms/step - loss: 3.9790 - tf_lwlrap: 0.4122 - val_loss: 3.9335 - val_tf_lwlrap: 0.3690\n",
      "Epoch 2/3\n",
      "5872/5872 [==============================] - 92s 16ms/step - loss: 3.6929 - tf_lwlrap: 0.4583 - val_loss: 3.9301 - val_tf_lwlrap: 0.3718\n",
      "Epoch 3/3\n",
      "5872/5872 [==============================] - 92s 16ms/step - loss: 3.5159 - tf_lwlrap: 0.4887 - val_loss: 3.9417 - val_tf_lwlrap: 0.3814\n",
      "Train on 5872 samples, validate on 5750 samples\n",
      "Epoch 1/3\n",
      "5872/5872 [==============================] - 91s 16ms/step - loss: 3.8264 - tf_lwlrap: 0.4006 - val_loss: 3.9581 - val_tf_lwlrap: 0.3748\n",
      "Epoch 2/3\n",
      "5872/5872 [==============================] - 91s 15ms/step - loss: 3.5068 - tf_lwlrap: 0.4559 - val_loss: 3.9686 - val_tf_lwlrap: 0.3855\n",
      "Epoch 3/3\n",
      "5872/5872 [==============================] - 91s 15ms/step - loss: 3.3424 - tf_lwlrap: 0.4871 - val_loss: 3.9488 - val_tf_lwlrap: 0.3813\n",
      "Train on 5888 samples, validate on 5866 samples\n",
      "Epoch 1/3\n",
      "5888/5888 [==============================] - 92s 16ms/step - loss: 3.6482 - tf_lwlrap: 0.4481 - val_loss: 3.8180 - val_tf_lwlrap: 0.4001\n",
      "Epoch 2/3\n",
      "5888/5888 [==============================] - 92s 16ms/step - loss: 3.3704 - tf_lwlrap: 0.4976 - val_loss: 3.8592 - val_tf_lwlrap: 0.4005\n",
      "Epoch 3/3\n",
      "5888/5888 [==============================] - 93s 16ms/step - loss: 3.2000 - tf_lwlrap: 0.5336 - val_loss: 3.9525 - val_tf_lwlrap: 0.3944\n",
      "Train on 5878 samples, validate on 5750 samples\n",
      "Epoch 1/3\n",
      "5878/5878 [==============================] - 91s 15ms/step - loss: 3.8195 - tf_lwlrap: 0.4308 - val_loss: 3.8397 - val_tf_lwlrap: 0.3971\n",
      "Epoch 2/3\n",
      "5878/5878 [==============================] - 91s 15ms/step - loss: 3.5034 - tf_lwlrap: 0.4890 - val_loss: 3.9066 - val_tf_lwlrap: 0.3941\n",
      "Epoch 3/3\n",
      "5878/5878 [==============================] - 91s 15ms/step - loss: 3.3300 - tf_lwlrap: 0.5186 - val_loss: 4.0255 - val_tf_lwlrap: 0.3785\n",
      "Train on 5871 samples, validate on 5750 samples\n",
      "Epoch 1/3\n",
      "5871/5871 [==============================] - 91s 16ms/step - loss: 3.7219 - tf_lwlrap: 0.4591 - val_loss: 3.9371 - val_tf_lwlrap: 0.3937\n",
      "Epoch 2/3\n",
      "5871/5871 [==============================] - 91s 15ms/step - loss: 3.3793 - tf_lwlrap: 0.5234 - val_loss: 3.9087 - val_tf_lwlrap: 0.3943\n",
      "Epoch 3/3\n",
      "5871/5871 [==============================] - 91s 15ms/step - loss: 3.1738 - tf_lwlrap: 0.5614 - val_loss: 4.0808 - val_tf_lwlrap: 0.3891\n",
      "Train on 5429 samples, validate on 5750 samples\n",
      "Epoch 1/3\n",
      "3904/5429 [====================>.........] - ETA: 17s - loss: 3.8010 - tf_lwlrap: 0.4324"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for n in range(epochs):\n",
    "    pick = random.sample(range(8),8)\n",
    "    for m in pick:\n",
    "        with open('preprocessed_dataset/noisy_train_arr_{}.pickle'.format(m), 'rb') as f:\n",
    "            X_train = pickle.load(f)\n",
    "            y_train = pickle.load(f)\n",
    "        pick_val = random.sample(range(2),1)[0]\n",
    "        with open('preprocessed_dataset/noisy_val_arr_{}.pickle'.format(pick_val), 'rb') as f:\n",
    "            X_val = pickle.load(f)\n",
    "            y_val = pickle.load(f)\n",
    "        model.fit(X_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=3,\n",
    "                  validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('20190510model.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "\n",
    "filename = glob.glob(TEST + \"*\")\n",
    "\n",
    "for file in filename:\n",
    "    wavfile = file\n",
    "    y_proc, sr = librosa.load(wavfile)\n",
    "    S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    X_proc = (log_S + 80) / 40 - 1\n",
    "    \n",
    "    num_div = X_proc.shape[1] // len_div\n",
    "    num_pad = len_div - X_proc.shape[1] % len_div\n",
    "    redidual_amp = np.zeros([num_freq, num_pad])\n",
    "    dum = np.hstack([X_proc, redidual_amp])\n",
    "    X_test_list.append(np.array(np.split(dum, num_div+1,1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/test_arr.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test_list, f)\n",
    "    pickle.dump(filename, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/test_arr.pickle', 'rb') as f:\n",
    "    X_test_list = pickle.load(f)\n",
    "    filename = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_list = []\n",
    "\n",
    "# for file in filename:\n",
    "#     wavfile = file\n",
    "#     y_proc, sr = librosa.load(wavfile)\n",
    "#     S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "#     log_S = librosa.power_to_db(S, ref=np.max)\n",
    "#     X_proc = (log_S + 80) / 40 - 1\n",
    "    \n",
    "#     num_div = X_proc.shape[1] // len_div\n",
    "#     num_pad = len_div - X_proc.shape[1] % len_div\n",
    "#     redidual_amp = np.zeros([num_freq, num_pad])\n",
    "#     dum = np.hstack([X_proc, redidual_amp])\n",
    "#     X_test_list.append(np.array(np.split(dum, num_div+1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for X_test in X_test_list:\n",
    "    pred = model.predict(X_test.reshape([-1, num_freq, len_div,1])).sum(axis=0) / len(X_test)\n",
    "    pred_list.append(pred)\n",
    "y_pred = np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_names = sample.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for f in filename:\n",
    "    names.append(f.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_file = pd.Series(names, name='fname')\n",
    "label = pd.DataFrame(y_pred, columns=sound_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.concat([se_file, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
