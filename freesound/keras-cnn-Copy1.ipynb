{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split \n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_curated.csv', 'train_noisy.csv', 'sample_submission.csv', 'train_noisy', 'train_curated', 'test']\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = \"input/\"\n",
    "# INPUT_FOLDER = \"../input/\"\n",
    "print(os.listdir(INPUT_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CURATED_PATH = INPUT_FOLDER + \"train_curated.csv\"\n",
    "TRAIN_NOISY_PATH = INPUT_FOLDER + \"train_noisy.csv\"\n",
    "SAMPLE_SUBMISSION_PATH = INPUT_FOLDER + \"sample_submission.csv\"\n",
    "TRAIN_CURATED = INPUT_FOLDER + \"train_curated/\"\n",
    "TRAIN_NOISY = INPUT_FOLDER + \"train_noisy/\"\n",
    "TEST = INPUT_FOLDER + \"test/\"\n",
    "\n",
    "train_curated = pd.read_csv(TRAIN_CURATED_PATH)\n",
    "train_noisy = pd.read_csv(TRAIN_NOISY_PATH)\n",
    "sample = pd.read_csv(SAMPLE_SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, src_dict):\n",
    "    ar = np.zeros([len(labels), len(src_dict)])\n",
    "    for i, label in enumerate(labels):\n",
    "        label_list = label.split(',')\n",
    "        for la in label_list:\n",
    "            ar[i, src_dict[la]] = 1\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = sample.columns[1:]\n",
    "num_targets = len(target_names)\n",
    "\n",
    "src_dict = {target_names[i]:i for i in range(num_targets)}\n",
    "src_dict_inv = {i:target_names[i] for i in range(num_targets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_freq = 128\n",
    "len_div = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = train_curated['fname'].values\n",
    "\n",
    "div = 500\n",
    "num_batch = len(file_name) // div\n",
    "rest = len(file_name) % div\n",
    "\n",
    "pos = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "pos.append(range(div*num_batch, div*num_batch+rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is done.\n",
      "iter No.1 is done.\n",
      "iter No.2 is done.\n",
      "iter No.3 is done.\n",
      "iter No.4 is done.\n",
      "iter No.5 is done.\n",
      "iter No.6 is done.\n",
      "iter No.7 is done.\n",
      "iter No.8 is done.\n",
      "iter No.9 is done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_proc_tmp = one_hot(train_curated['labels'], src_dict)\n",
    "\n",
    "for k in range(num_batch+1):\n",
    "    X_proc_ = np.zeros([1, num_freq, len_div])\n",
    "    y_proc_ = np.zeros([1,80])\n",
    "    for i, file in enumerate(file_name[pos[k]]):\n",
    "        wavfile = TRAIN_CURATED + file\n",
    "        y_proc, sr = librosa.load(wavfile)\n",
    "        S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "        X_proc = (log_S + 80) / 40 - 1\n",
    "\n",
    "        num_div = X_proc.shape[1] // len_div\n",
    "        num_pad = len_div - X_proc.shape[1] % len_div\n",
    "        redidual_amp = np.zeros([num_freq, num_pad])\n",
    "        dum = np.hstack([X_proc, redidual_amp])\n",
    "        X_proc_ = np.vstack([X_proc_, np.array(np.split(dum, num_div+1,1))])\n",
    "        for _ in range(num_div+1):\n",
    "            y_proc_ = np.vstack([y_proc_, y_proc_tmp[i+div*k]])\n",
    "\n",
    "    X = X_proc_[1:]\n",
    "    y = y_proc_[1:]\n",
    "    X = X.reshape([-1, num_freq, len_div, 1])\n",
    "    \n",
    "    print('iter No.{} is done.'.format(k))\n",
    "\n",
    "    with open('out/train_arr_{}.pickle'.format(k), 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9377, 128, 256, 1)\n",
      "(9377, 80)\n"
     ]
    }
   ],
   "source": [
    "X_proc_ = np.zeros([1, num_freq, len_div, 1])\n",
    "y_proc_ = np.zeros([1,80])\n",
    "\n",
    "for k in range(num_batch+1):\n",
    "    with open('out/train_arr_{}.pickle'.format(k), 'rb') as f:\n",
    "        X_part = pickle.load(f)\n",
    "        y_part = pickle.load(f)\n",
    "\n",
    "    X_proc_ = np.vstack([X_proc_, X_part])\n",
    "    y_proc_ = np.vstack([y_proc_, y_part])\n",
    "    \n",
    "X = X_proc_[1:]\n",
    "y = y_proc_[1:]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "with open('train_arr.pickle', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_arr.pickle', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is done.\n",
      "iter No.1 is done.\n",
      "iter No.2 is done.\n",
      "iter No.3 is done.\n",
      "iter No.4 is done.\n",
      "iter No.5 is done.\n",
      "iter No.6 is done.\n",
      "iter No.7 is done.\n",
      "iter No.8 is done.\n",
      "iter No.9 is done.\n",
      "iter No.10 is done.\n",
      "iter No.11 is done.\n",
      "iter No.12 is done.\n",
      "iter No.13 is done.\n",
      "iter No.14 is done.\n",
      "iter No.15 is done.\n",
      "iter No.16 is done.\n",
      "iter No.17 is done.\n",
      "iter No.18 is done.\n",
      "iter No.19 is done.\n",
      "iter No.20 is done.\n",
      "iter No.21 is done.\n",
      "iter No.22 is done.\n",
      "iter No.23 is done.\n",
      "iter No.24 is done.\n",
      "iter No.25 is done.\n",
      "iter No.26 is done.\n",
      "iter No.27 is done.\n",
      "iter No.28 is done.\n",
      "iter No.29 is done.\n",
      "iter No.30 is done.\n",
      "iter No.31 is done.\n",
      "iter No.32 is done.\n",
      "iter No.33 is done.\n",
      "iter No.34 is done.\n",
      "iter No.35 is done.\n",
      "iter No.36 is done.\n",
      "iter No.37 is done.\n",
      "iter No.38 is done.\n",
      "iter No.39 is done.\n"
     ]
    }
   ],
   "source": [
    "file_name = train_noisy['fname'].values\n",
    "\n",
    "div = 500\n",
    "num_batch = len(file_name) // div\n",
    "rest = len(file_name) % div\n",
    "\n",
    "pos = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "pos.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "\n",
    "y_proc_tmp = one_hot(train_noisy['labels'], src_dict)\n",
    "\n",
    "for k in range(num_batch+1):\n",
    "    X_proc_ = np.zeros([1, num_freq, len_div])\n",
    "    y_proc_ = np.zeros([1,80])\n",
    "    for i, file in enumerate(file_name[pos[k]]):\n",
    "        wavfile = TRAIN_NOISY + file\n",
    "        y_proc, sr = librosa.load(wavfile)\n",
    "        S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "        X_proc = (log_S + 80) / 40 - 1\n",
    "\n",
    "        num_div = X_proc.shape[1] // len_div\n",
    "        num_pad = len_div - X_proc.shape[1] % len_div\n",
    "        redidual_amp = np.zeros([num_freq, num_pad])\n",
    "        dum = np.hstack([X_proc, redidual_amp])\n",
    "        X_proc_ = np.vstack([X_proc_, np.array(np.split(dum, num_div+1,1))])\n",
    "        for _ in range(num_div+1):\n",
    "            y_proc_ = np.vstack([y_proc_, y_proc_tmp[i+div*k]])\n",
    "\n",
    "    X = X_proc_[1:]\n",
    "    y = y_proc_[1:]\n",
    "    X = X.reshape([-1, num_freq, len_div, 1])\n",
    "    \n",
    "    print('iter No.{} is done.'.format(k))\n",
    "\n",
    "    with open('out/train_noisy_arr_{}.pickle'.format(k), 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "        pickle.dump(y, f)\n",
    "        \n",
    "X_proc_ = np.zeros([1, num_freq, len_div, 1])\n",
    "y_proc_ = np.zeros([1,80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num_batch+1):\n",
    "    with open('out/train_noisy_arr_{}.pickle'.format(k), 'rb') as f:\n",
    "        X_part = pickle.load(f)\n",
    "        y_part = pickle.load(f)\n",
    "\n",
    "    X_proc_ = np.vstack([X_proc_, X_part])\n",
    "    y_proc_ = np.vstack([y_proc_, y_part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_proc_[1:]\n",
    "y = y_proc_[1:]\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "with open('train_noisy_arr.pickle', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_noisy_arr.pickle', 'rb') as f:\n",
    "    X_noisy = pickle.load(f)\n",
    "    y_noisy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(num_freq,len_div,1), name='input')\n",
    "\n",
    "dense_list = []\n",
    "\n",
    "## Block 1\n",
    "conv1 = Conv2D(4, (19, 19),activation='relu',padding='same',name='conv1')(inputs)\n",
    "pool1 = MaxPooling2D((19, 19),strides=(1, 1),padding='same',name='pool1')(conv1)\n",
    "norm1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm1')(pool1)\n",
    "drop1 = Dropout(rate=0.05)(norm1)\n",
    "\n",
    "conv1_1 = Conv2D(4, (11, 11),activation='relu',padding='same',name='conv1_1')(drop1)\n",
    "pool1_1 = MaxPooling2D((5, 5),strides=(5, 5),padding='same',name='pool1_1')(conv1_1)\n",
    "norm1_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm1_1')(pool1_1)\n",
    "drop1_1 = Dropout(rate=0.05)(norm1_1)\n",
    "\n",
    "flatten1 = Flatten(name='flatten1')(drop1)\n",
    "dense1 = Dense(16, name='dense1')(flatten1)\n",
    "act1 = Activation('relu',name='act1')(dense1)\n",
    "dense_list.append(act1)\n",
    "\n",
    "## Block 2\n",
    "conv2 = Conv2D(4, (13, 13),activation='relu',padding='same',name='conv2')(inputs)\n",
    "pool2 = MaxPooling2D((13, 13), strides=(1, 1), padding='same',name='pool2')(conv2)\n",
    "norm2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm2')(pool2)\n",
    "drop2 = Dropout(rate=0.05)(norm2)\n",
    "\n",
    "conv2_1 = Conv2D(4, (7, 7),activation='relu',padding='same',name='conv2_1')(drop2)\n",
    "pool2_1 = MaxPooling2D((7, 7), strides=(5, 5), padding='same',name='pool2_1')(conv2_1)\n",
    "norm2_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001,name='norm2_1')(pool2_1)\n",
    "drop2_1 = Dropout(rate=0.05)(norm2_1)\n",
    "\n",
    "flatten2 = Flatten(name='flatten2')(drop2_1)\n",
    "dense2 = Dense(16, name='dense2')(flatten2)\n",
    "act2 = Activation('relu',name='act2')(dense2)\n",
    "dense_list.append(act2)\n",
    "\n",
    "## Block 3\n",
    "conv3 = Conv2D(8, (11, 11), activation='relu',padding='same',name='conv3')(inputs)\n",
    "pool3 = MaxPooling2D((11, 11), strides=(2, 2), padding='same',name='pool3')(conv3)\n",
    "norm3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm3')(pool3)\n",
    "drop3 = Dropout(rate=0.05)(norm3)\n",
    "\n",
    "conv3_1 = Conv2D(8, (5, 5), activation='relu',padding='same',name='conv3_1')(drop3)\n",
    "pool3_1 = MaxPooling2D((5, 5), strides=(2, 2), padding='same',name='pool3_1')(conv3_1)\n",
    "norm3_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm3_1')(pool3_1)\n",
    "drop3_1 = Dropout(rate=0.05)(norm3_1)\n",
    "\n",
    "flatten3 = Flatten(name='flatten3')(drop3_1)\n",
    "dense3 = Dense(16, name='dense3')(flatten3)\n",
    "act3 = Activation('relu',name='act3')(dense3)\n",
    "dense_list.append(act3)\n",
    "\n",
    "## Block 4\n",
    "conv4 = Conv2D(8, (9, 9),activation='relu',padding='same',name='conv4')(inputs)\n",
    "pool4 = MaxPooling2D((9, 9), strides=(2, 2), padding='same',name='pool4')(conv4)\n",
    "norm4 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm4')(pool4)\n",
    "drop4 = Dropout(rate=0.05)(norm4)\n",
    "\n",
    "conv4_1 = Conv2D(8, (3, 3),activation='relu',padding='same',name='conv4_1')(drop4)\n",
    "pool4_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='same',name='pool4_1')(conv4_1)\n",
    "norm4_1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.0001,name='norm4_1')(pool4_1)\n",
    "drop4_1 = Dropout(rate=0.05)(norm4_1)\n",
    "\n",
    "flatten4 = Flatten(name='flatten4')(drop4_1)\n",
    "dense4 = Dense(16, name='dense4')(flatten4)\n",
    "act4 = Activation('relu',name='act4')(dense4)\n",
    "dense_list.append(act4)\n",
    "\n",
    "concat = concatenate(dense_list, name='concat', axis=1)\n",
    "\n",
    "dense2 = Dense(80, name='dense_all')(concat)\n",
    "pred = Activation('softmax',name='pred')(dense2)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_one_sample_positive_class_precisions(y_true, y_pred) :\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    # find true labels\n",
    "    pos_class_indices = tf.where(y_true > 0) \n",
    "    \n",
    "    # put rank on each element\n",
    "    class_rankings = tf.nn.top_k(y_pred, k=num_classes).indices \n",
    "    \n",
    "    #pick_up ranks\n",
    "    num_correct_until_correct = tf.gather_nd(class_rankings, pos_class_indices) \n",
    "    \n",
    "    # add one for division for \"presicion_at_hits\"\n",
    "    num_correct_until_correct_one = tf.add(num_correct_until_correct, 1) \n",
    "    num_correct_until_correct_one = tf.cast(num_correct_until_correct_one, tf.float32)\n",
    "    \n",
    "    # generate tensor [num_sample, predict_rank], \n",
    "    # top-N predicted elements have flag, N is the number of positive for each sample.\n",
    "    sample_label = pos_class_indices[:, 0]   \n",
    "    sample_label = tf.reshape(sample_label, (-1, 1))\n",
    "    sample_label = tf.cast(sample_label, tf.int32)\n",
    "    num_correct_until_correct = tf.reshape(num_correct_until_correct, (-1, 1))  \n",
    "    retrieved_class_true_position = tf.concat((sample_label, \n",
    "                                               num_correct_until_correct), axis=1)\n",
    "    retrieved_pos = tf.ones(shape=tf.shape(retrieved_class_true_position)[0], dtype=tf.int32)\n",
    "    retrieved_class_true = tf.scatter_nd(retrieved_class_true_position, \n",
    "                                         retrieved_pos, \n",
    "                                         tf.shape(y_pred))\n",
    "\n",
    "    # cumulate predict_rank\n",
    "    retrieved_cumulative_hits = tf.cumsum(retrieved_class_true, axis=1)\n",
    "\n",
    "    # find positive position\n",
    "    pos_ret_indices = tf.where(retrieved_class_true > 0)\n",
    "\n",
    "    # find cumulative hits\n",
    "    correct_rank = tf.gather_nd(retrieved_cumulative_hits, pos_ret_indices)  \n",
    "    correct_rank = tf.cast(correct_rank, tf.float32)\n",
    "\n",
    "    # compute presicion\n",
    "    precision_at_hits = tf.truediv(correct_rank, num_correct_until_correct_one)\n",
    " \n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def tf_lwlrap(y_true, y_pred):\n",
    "    num_samples, num_classes = y_pred.shape\n",
    "    \n",
    "    pos_class_indices, precision_at_hits = (tf_one_sample_positive_class_precisions(y_true, y_pred))\n",
    "    \n",
    "    pos_flgs = tf.cast(y_true > 0, tf.int32)\n",
    "    \n",
    "    labels_per_class = tf.reduce_sum(pos_flgs, axis=0)\n",
    "    \n",
    "    weight_per_class = tf.truediv(tf.cast(labels_per_class, tf.float32),\n",
    "                                  tf.cast(tf.reduce_sum(labels_per_class), tf.float32))\n",
    "    \n",
    "    sum_precisions_by_classes = tf.zeros(shape=(num_classes), dtype=tf.float32)  \n",
    "    \n",
    "    class_label = pos_class_indices[:,1]\n",
    "\n",
    "    sum_precisions_by_classes = tf.unsorted_segment_sum(precision_at_hits,\n",
    "                                                        class_label,\n",
    "                                                       num_classes)\n",
    "    \n",
    "    labels_per_class = tf.cast(labels_per_class, tf.float32)\n",
    "    labels_per_class = tf.add(labels_per_class, 1e-7)\n",
    "    per_class_lwlrap = tf.truediv(sum_precisions_by_classes,\n",
    "                                  tf.cast(labels_per_class, tf.float32))\n",
    "    \n",
    "    out = tf.cast(tf.tensordot(per_class_lwlrap, weight_per_class, axes=1), dtype=tf.float32)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "           rotation_range=0,\n",
    "           width_shift_range=16,\n",
    "           height_shift_range=0,\n",
    "           shear_range=0,\n",
    "           zoom_range=0,\n",
    "           horizontal_flip=False,\n",
    "           vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "294/293 [==============================] - 114s 387ms/step - loss: 3.7492 - acc: 0.2166\n",
      "Epoch 2/30\n",
      "294/293 [==============================] - 111s 378ms/step - loss: 3.1205 - acc: 0.3089\n",
      "Epoch 3/30\n",
      "294/293 [==============================] - 111s 377ms/step - loss: 2.9840 - acc: 0.3394\n",
      "Epoch 4/30\n",
      "294/293 [==============================] - 112s 382ms/step - loss: 2.8626 - acc: 0.3560\n",
      "Epoch 5/30\n",
      "294/293 [==============================] - 113s 385ms/step - loss: 2.7812 - acc: 0.3793\n",
      "Epoch 6/30\n",
      "210/293 [====================>.........] - ETA: 31s - loss: 2.7522 - acc: 0.3738"
     ]
    }
   ],
   "source": [
    "with open('train_arr.pickle', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(num_batch+1):\n",
    "    with open('out/train_noisy_arr_{}.pickle'.format(k), 'rb') as f:\n",
    "        X_train = pickle.load(f)\n",
    "        y_train = pickle.load(f)\n",
    "\n",
    "    datagen.fit(X_train)\n",
    "    model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                        steps_per_epoch=len(X_train) / 32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20190505model_curated_noisy.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 300\n",
    "num_batch = len(file_name) // div\n",
    "rest = len(file_name) % div\n",
    "\n",
    "pos = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "pos.append(range(div*num_batch, div*num_batch+rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(0, 300), range(300, 600), range(600, 900), range(900, 1120)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is done.\n",
      "iter No.1 is done.\n",
      "iter No.2 is done.\n",
      "iter No.3 is done.\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_batch+1):\n",
    "    X_proc_ = []\n",
    "    for file in file_name[pos[k]]:\n",
    "        wavfile = file\n",
    "        y_proc, sr = librosa.load(wavfile)\n",
    "        S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "        log_S = librosa.power_to_db(S, ref=np.max)\n",
    "        X_proc = (log_S + 80) / 40 - 1\n",
    "\n",
    "        num_div = X_proc.shape[1] // len_div\n",
    "        num_pad = len_div - X_proc.shape[1] % len_div\n",
    "        redidual_amp = np.zeros([num_freq, num_pad])\n",
    "        dum = np.hstack([X_proc, redidual_amp])\n",
    "        X_proc_.append(np.array(np.split(dum, num_div+1,1)))\n",
    "    \n",
    "    print('iter No.{} is done.'.format(k))\n",
    "\n",
    "    with open('out/test_arr_{}.pickle'.format(k), 'wb') as f:\n",
    "        pickle.dump(X_proc_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc_ = []\n",
    "\n",
    "for k in range(num_batch+1):\n",
    "    with open('out/test_arr_{}.pickle'.format(k), 'rb') as f:\n",
    "        X_part = pickle.load(f)\n",
    "    X_proc_.extend(X_part)\n",
    "    \n",
    "with open('test_arr.pickle', 'wb') as f:\n",
    "    pickle.dump(X_proc_, f)\n",
    "    pickle.dump(file_name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_proc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_list = []\n",
    "\n",
    "for file in filename:\n",
    "    wavfile = file\n",
    "    y_proc, sr = librosa.load(wavfile)\n",
    "    S = librosa.feature.melspectrogram(y_proc, sr=sr, n_mels=num_freq)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    X_proc = (log_S + 80) / 40 - 1\n",
    "    \n",
    "    num_div = X_proc.shape[1] // len_div\n",
    "    num_pad = len_div - X_proc.shape[1] % len_div\n",
    "    redidual_amp = np.zeros([num_freq, num_pad])\n",
    "    dum = np.hstack([X_proc, redidual_amp])\n",
    "    X_test_list.append(np.array(np.split(dum, num_div+1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for X_test in X_test_list:\n",
    "    pred = model.predict(X_test.reshape([-1, num_freq, len_div,1])).sum(axis=0) / len(X_test)\n",
    "    pred_list.append(pred)\n",
    "y_pred = np.array(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_names = sample.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for f in filename:\n",
    "    names.append(f.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_file = pd.Series(names, name='fname')\n",
    "label = pd.DataFrame(y_pred, columns=sound_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.concat([se_file, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
