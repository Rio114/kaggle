{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librosa for audio\n",
    "import librosa\n",
    "# And the display module for visualization\n",
    "import librosa.display\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_curated.csv', 'train_noisy.csv', 'sample_submission.csv', 'train_noisy', 'train_curated', 'test']\n"
     ]
    }
   ],
   "source": [
    "FOLDER = \"../../data_kaggle/freesound/\"\n",
    "PREPROCESS = FOLDER + \"preprocessed_dataset/\"\n",
    "OUTPUT = FOLDER + \"out/\"\n",
    "INPUT_FOLDER = FOLDER + \"input/\"\n",
    "# INPUT_FOLDER = \"../input\" # for kaggle\n",
    "print(os.listdir(INPUT_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CURATED_PATH = INPUT_FOLDER + \"train_curated.csv\"\n",
    "TRAIN_NOISY_PATH = INPUT_FOLDER + \"train_noisy.csv\"\n",
    "SAMPLE_SUBMISSION_PATH = INPUT_FOLDER + \"sample_submission.csv\"\n",
    "TRAIN_NOISY = INPUT_FOLDER + \"train_noisy/\"\n",
    "TRAIN_CURATED = INPUT_FOLDER + \"train_curated/\"\n",
    "TEST = INPUT_FOLDER + \"test/\"\n",
    "\n",
    "train_noisy = pd.read_csv(TRAIN_NOISY_PATH)\n",
    "train_curated = pd.read_csv(TRAIN_CURATED_PATH)\n",
    "sample = pd.read_csv(SAMPLE_SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = sample.columns[1:]\n",
    "num_targets = len(target_names)\n",
    "\n",
    "target_dict = {target_names[i]:i for i in range(num_targets)}\n",
    "target_dict_inv = {i:target_names[i] for i in range(num_targets)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, target_dict):\n",
    "    ar = np.zeros([len(labels), len(target_dict)])\n",
    "    for i, label in enumerate(labels):\n",
    "        label_list = label.split(',')\n",
    "        for la in label_list:\n",
    "            ar[i, target_dict[la]] = 1\n",
    "    return ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_freq = 128\n",
    "len_div = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2np(wavfile,  n_mels=num_freq):\n",
    "    y, sr = librosa.load(wavfile)\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr, n_mels=n_mels)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max) #[-80, 0.0]\n",
    "    X = (log_S +80) / 80 # [0.0, 1.0]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3976\n",
      "994\n"
     ]
    }
   ],
   "source": [
    "file_name_curated = (TRAIN_CURATED + train_curated['fname'].values).reshape(-1,1)\n",
    "label_curated = one_hot(train_curated['labels'], target_dict)\n",
    "file_train_curated, file_val_curated, label_train_curated, label_val_curated = train_test_split(file_name_curated, label_curated, test_size=0.2, random_state=0)\n",
    "print(len(file_train_curated))\n",
    "print(len(file_val_curated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is being processed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_train_curated\n",
    "files = file_train_curated\n",
    "out_name = 'train_curated_{}.pickle'\n",
    "\n",
    "div = len(files)\n",
    "num_batch = len(files) // div\n",
    "rest = len(files) % div\n",
    "\n",
    "slices = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "slices.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "s = slices[0]\n",
    "k = 0\n",
    "X_temp = np.zeros([1, len_div, num_freq])\n",
    "y_temp = np.zeros([1, num_targets])\n",
    "print('iter No.{} is being processed.'.format(k))\n",
    "for fname, label in zip(files[s].reshape(-1), labels[s]):\n",
    "    X = file2np(fname)\n",
    "\n",
    "    num_div = X.shape[1] // len_div\n",
    "    num_pad = len_div - X.shape[1] % len_div\n",
    "    X_redidual = np.zeros([num_freq, num_pad])\n",
    "    X_padded = np.hstack([X, X_redidual]).T\n",
    "    X_temp = np.vstack([X_temp, np.array(np.split(X_padded, num_div+1))])\n",
    "    for _ in range(num_div+1):\n",
    "        y_temp = np.vstack([y_temp, label])\n",
    "\n",
    "X_processed = X_temp[1:]\n",
    "y_processed = y_temp[1:]\n",
    "\n",
    "with open(PREPROCESS+out_name.format(k), 'wb') as f:\n",
    "    pickle.dump(X_processed, f)\n",
    "    pickle.dump(y_processed, f)\n",
    "del X_processed, y_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is being processed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_val_curated\n",
    "files = file_val_curated\n",
    "out_name = 'val_curated_{}.pickle'\n",
    "\n",
    "div = len(files)\n",
    "num_batch = len(files) // div\n",
    "rest = len(files) % div\n",
    "\n",
    "slices = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "slices.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "s = slices[0]\n",
    "k = 0\n",
    "X_temp = np.zeros([1, len_div, num_freq])\n",
    "y_temp = np.zeros([1, num_targets])\n",
    "print('iter No.{} is being processed.'.format(k))\n",
    "for fname, label in zip(files[s].reshape(-1), labels[s]):\n",
    "    X = file2np(fname)\n",
    "\n",
    "    num_div = X.shape[1] // len_div\n",
    "    num_pad = len_div - X.shape[1] % len_div\n",
    "    X_redidual = np.zeros([num_freq, num_pad])\n",
    "    X_padded = np.hstack([X, X_redidual]).T\n",
    "    X_temp = np.vstack([X_temp, np.array(np.split(X_padded, num_div+1))])\n",
    "    for _ in range(num_div+1):\n",
    "        y_temp = np.vstack([y_temp, label])\n",
    "\n",
    "X_processed = X_temp[1:]\n",
    "y_processed = y_temp[1:]\n",
    "\n",
    "with open(PREPROCESS+out_name.format(k), 'wb') as f:\n",
    "    pickle.dump(X_processed, f)\n",
    "    pickle.dump(y_processed, f)\n",
    "del X_processed, y_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17833\n",
      "1982\n"
     ]
    }
   ],
   "source": [
    "file_name_noisy = (TRAIN_NOISY + train_noisy['fname'].values).reshape(-1,1)\n",
    "label_noisy = one_hot(train_noisy['labels'], target_dict)\n",
    "file_train_noisy, file_val_noisy, label_train_noisy, label_val_noisy = train_test_split(file_name_noisy, label_noisy, test_size=0.1, random_state=0)\n",
    "print(len(file_train_noisy))\n",
    "print(len(file_val_noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is being processed.\n"
     ]
    }
   ],
   "source": [
    "labels = label_train_noisy\n",
    "files = file_train_noisy\n",
    "out_name = 'train_noisy_{}.pickle'\n",
    "\n",
    "div = 4000\n",
    "num_batch = len(files) // div\n",
    "rest = len(files) % div\n",
    "\n",
    "slices = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "slices.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "for k, s in enumerate(slices):\n",
    "    X_temp = np.zeros([1, len_div, num_freq])\n",
    "    y_temp = np.zeros([1, num_targets])\n",
    "    print('iter No.{} is being processed.'.format(k))\n",
    "    for fname, label in zip(files[s].reshape(-1), labels[s]):\n",
    "        X = file2np(fname)\n",
    "\n",
    "        num_div = X.shape[1] // len_div\n",
    "        num_pad = len_div - X.shape[1] % len_div\n",
    "        X_redidual = np.zeros([num_freq, num_pad])\n",
    "        X_padded = np.hstack([X, X_redidual]).T\n",
    "        X_temp = np.vstack([X_temp, np.array(np.split(X_padded, num_div+1))])\n",
    "        for _ in range(num_div+1):\n",
    "            y_temp = np.vstack([y_temp, label])\n",
    "            \n",
    "    X_processed = X_temp[1:]\n",
    "    y_processed = y_temp[1:]\n",
    "    \n",
    "    with open(PREPROCESS+out_name.format(k), 'wb') as f:\n",
    "        pickle.dump(X_processed, f)\n",
    "        pickle.dump(y_processed, f)\n",
    "    del X_processed, y_processed\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_val_noisy\n",
    "files = file_val_noisy\n",
    "out_name = 'val_noisy_{}.pickle'\n",
    "\n",
    "div = len(files)\n",
    "num_batch = len(files) // div\n",
    "rest = len(files) % div\n",
    "\n",
    "slices = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "slices.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "s = slices[0]\n",
    "k = 0\n",
    "X_temp = np.zeros([1, len_div, num_freq])\n",
    "y_temp = np.zeros([1, num_targets])\n",
    "print('iter No.{} is being processed.'.format(k))\n",
    "for fname, label in zip(files[s].reshape(-1), labels[s]):\n",
    "    X = file2np(fname)\n",
    "\n",
    "    num_div = X.shape[1] // len_div\n",
    "    num_pad = len_div - X.shape[1] % len_div\n",
    "    X_redidual = np.zeros([num_freq, num_pad])\n",
    "    X_padded = np.hstack([X, X_redidual]).T\n",
    "    X_temp = np.vstack([X_temp, np.array(np.split(X_padded, num_div+1))])\n",
    "    for _ in range(num_div+1):\n",
    "        y_temp = np.vstack([y_temp, label])\n",
    "\n",
    "X_processed = X_temp[1:]\n",
    "y_processed = y_temp[1:]\n",
    "\n",
    "with open(PREPROCESS+out_name.format(k), 'wb') as f:\n",
    "    pickle.dump(X_processed, f)\n",
    "    pickle.dump(y_processed, f)\n",
    "del X_processed, y_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter No.0 is being processed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sample['fname'].values\n",
    "folder = TEST\n",
    "out_name = 'test_arr_{}.pickle'\n",
    "file_list = []\n",
    "\n",
    "div = len(files)\n",
    "num_batch = len(files) // div\n",
    "rest = len(files) % div\n",
    "\n",
    "slices = [range(div*k, div*(k+1)) for k in range(num_batch)]\n",
    "slices.append(range(div*num_batch, div*num_batch+rest))\n",
    "\n",
    "s = slices[0]\n",
    "k = 0\n",
    "X_temp = np.zeros([1, len_div, num_freq])\n",
    "print('iter No.{} is being processed.'.format(k))\n",
    "for fname in files[s].reshape(-1):\n",
    "    X = file2np(TEST + fname)\n",
    "\n",
    "    num_div = X.shape[1] // len_div\n",
    "    num_pad = len_div - X.shape[1] % len_div\n",
    "    X_redidual = np.zeros([num_freq, num_pad])\n",
    "    X_padded = np.hstack([X, X_redidual]).T\n",
    "    X_temp = np.vstack([X_temp, np.array(np.split(X_padded, num_div+1))])\n",
    "\n",
    "X_processed = X_temp[1:]\n",
    "\n",
    "with open(PREPROCESS+out_name.format(k), 'wb') as f:\n",
    "    pickle.dump(X_processed, f)\n",
    "    pickle.dump(file_list, f)\n",
    "del X_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
